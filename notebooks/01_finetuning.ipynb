{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c723bd58",
   "metadata": {},
   "source": [
    "## Fine-Tuning the Gemma 3-4B Model with LoRA using UnsLoTH\n",
    "\n",
    "Fine-tuning large language models (LLMs) is often computationally expensive and operationally complex. UnsLoTH simplifies this process by providing highly optimized training utilities, enabling efficient fine-tuning even for multi-billion-parameter models.\n",
    "\n",
    "In this notebook, we demonstrate how to fine-tune the Gemma 3-4B model using Low-Rank Adaptation (LoRA), a parameter-efficient fine-tuning technique that significantly reduces memory usage and training cost while preserving model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1525125",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "\n",
    "Before running this notebook, ensure the following requirements are met:\n",
    "- Hardware: Access to a GPU-enabled machine (e.g., NVIDIA A100, V100, or equivalent).\n",
    "- Software: The UnsLoTH library and its dependencies must be installed (installation steps are provided below).\n",
    "- Credentials:\n",
    "    - Proper authentication for Google Cloud Platform (GCP) if using a Cloud Storage bucket.\n",
    "    - Optional Weights & Biases (W&B) credentials for experiment tracking.\n",
    "\n",
    "### Important Notes\n",
    "- Model checkpoints and intermediate artifacts are stored directly in a Google Cloud Storage bucket. Ensure that your GCP credentials are correctly configured and that you have access to the target bucket.\n",
    "- Training metrics are logged using Weights & Biases (W&B) by default. If W&B logging is not required, it can be disabled in the training configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549eae36",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q accelerate datasets transformers bitsandbytes\n",
    "%pip install -q wandb trl unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1670fe17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yashpareek_workmail/unsloth-venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_17941/340567520.py:19: UserWarning: WARNING: Unsloth should be imported before transformers to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import FastModel, is_bfloat16_supported\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpareek-ml\u001b[0m (\u001b[33mpareek-ml-personal\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/yashpareek_workmail/unsloth_finetuning/notebooks/wandb/run-20251216_141232-hw4h2wal</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/pareek-ml-personal/instruct-rag-finetune/runs/hw4h2wal' target=\"_blank\">rosy-butterfly-8</a></strong> to <a href='https://wandb.ai/pareek-ml-personal/instruct-rag-finetune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/pareek-ml-personal/instruct-rag-finetune' target=\"_blank\">https://wandb.ai/pareek-ml-personal/instruct-rag-finetune</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/pareek-ml-personal/instruct-rag-finetune/runs/hw4h2wal' target=\"_blank\">https://wandb.ai/pareek-ml-personal/instruct-rag-finetune/runs/hw4h2wal</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n"
     ]
    }
   ],
   "source": [
    "# train_rag_unsloth.py\n",
    "import os\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    TrainingArguments,\n",
    "    TrainerCallback,\n",
    "    TrainerState,\n",
    "    TrainerControl,\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "import numpy as np\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "from unsloth import FastModel, is_bfloat16_supported\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "SEED = 3407\n",
    "# set random seed for reproducibility\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "MODEL_NAME = \"unsloth/gemma-3-4b-it\"  # Base model\n",
    "DATASET_NAME = \"FreedomIntelligence/RAG-Instruct\"  # HF dataset\n",
    "OUTPUT_DIR = \"./outputs/gemma3-4b-rag\"\n",
    "\n",
    "BUCKET_NAME = \"model-finetune-1\"  # GCP bucket for finetune checkpoints\n",
    "GCS_PREFIX = \"checkpoints/gemma3-4b-rag\"  # optional path inside bucket\n",
    "ENABLE_GCS_SYNC = True\n",
    "\n",
    "MAX_SEQ_LENGTH = 2048  # safe for 4B on L4 with packing\n",
    "MAX_DOCS = 2  # number of documents per sample\n",
    "MAX_DOC_CHARS = 4000  # truncate context to avoid huge prompts\n",
    "\n",
    "# Budget controls â€“ tune these first\n",
    "MAX_TRAIN_SAMPLES = None  # set to None for full dataset\n",
    "NUM_TRAIN_EPOCHS = 1\n",
    "LEARNING_RATE = 2e-4\n",
    "\n",
    "BATCH_SIZE = 1  # per-device batch size\n",
    "GRAD_ACCUM_STEPS = 4  # effective batch size = BATCH_SIZE * GRAD_ACCUM_STEPS\n",
    "\n",
    "USE_WANDB = True  # set True and export WANDB_PROJECT if you want logging\n",
    "WANDB_PROJECT = \"instruct-rag-finetune\"\n",
    "\n",
    "\n",
    "if USE_WANDB:\n",
    "    run = wandb.init(\n",
    "        entity=\"pareek-ml-personal\",\n",
    "        project=WANDB_PROJECT,\n",
    "        config={\n",
    "            \"model_name\": MODEL_NAME,\n",
    "            \"dataset_name\": DATASET_NAME,\n",
    "            \"max_train_samples\": MAX_TRAIN_SAMPLES,\n",
    "            \"num_train_epochs\": NUM_TRAIN_EPOCHS,\n",
    "            \"learning_rate\": LEARNING_RATE,\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"grad_accum_steps\": GRAD_ACCUM_STEPS,\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146a5855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# GCS SYNC CALLBACK\n",
    "# =========================\n",
    "\n",
    "\n",
    "class GCSSyncCallback(TrainerCallback):\n",
    "    def __init__(self, local_dir: str, bucket_name: str, prefix: str):\n",
    "        self.local_dir = os.path.abspath(local_dir)\n",
    "        self.bucket_name = bucket_name\n",
    "        self.prefix = prefix.rstrip(\"/\") if prefix else \"\"\n",
    "        self.client = storage.Client()\n",
    "        self.bucket = self.client.bucket(bucket_name)\n",
    "\n",
    "    def on_save(\n",
    "        self,\n",
    "        args: TrainingArguments,\n",
    "        state: TrainerState,\n",
    "        control: TrainerControl,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # Sync only the checkpoint that was just written\n",
    "        if not os.path.exists(self.local_dir):\n",
    "            return\n",
    "\n",
    "        print(\n",
    "            f\"[GCSSync] Syncing {self.local_dir} to gs://{self.bucket_name}/{self.prefix} ...\"\n",
    "        )\n",
    "        self._sync_directory(self.local_dir, self.prefix)\n",
    "        print(\"[GCSSync] Sync complete.\")\n",
    "        return control\n",
    "\n",
    "    def _sync_directory(self, local_dir: str, gcs_prefix: str):\n",
    "        for root, dirs, files in os.walk(local_dir):\n",
    "            for fname in files:\n",
    "                local_path = os.path.join(root, fname)\n",
    "                rel_path = os.path.relpath(local_path, local_dir)\n",
    "                blob_name = f\"{gcs_prefix}/{rel_path}\" if gcs_prefix else rel_path\n",
    "                blob = self.bucket.blob(blob_name)\n",
    "                blob.upload_from_filename(local_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ee2637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# MODEL & TOKENIZER\n",
    "# =========================\n",
    "\n",
    "\n",
    "def load_model_and_tokenizer():\n",
    "    print(\"Loading base model and tokenizer...\")\n",
    "    model, tokenizer = FastModel.from_pretrained(\n",
    "        model_name=MODEL_NAME,\n",
    "        max_seq_length=MAX_SEQ_LENGTH,\n",
    "        load_in_4bit=True,\n",
    "        load_in_8bit=False,\n",
    "        full_finetuning=False,\n",
    "    )\n",
    "\n",
    "    print(\"Applying LoRA (QLoRA) configuration...\")\n",
    "    model = FastModel.get_peft_model(\n",
    "        model,\n",
    "        finetune_vision_layers=False,\n",
    "        finetune_language_layers=True,\n",
    "        finetune_attention_modules=True,\n",
    "        finetune_mlp_modules=True,\n",
    "        r=8,  # LoRA rank\n",
    "        lora_alpha=8,  # usually >= r\n",
    "        lora_dropout=0,\n",
    "        bias=\"none\",\n",
    "        random_state=3407,\n",
    "    )\n",
    "\n",
    "    # Prepare for training (gradient checkpointing etc.)\n",
    "    FastModel.for_training(model)\n",
    "\n",
    "    # Attach Gemma-3 chat template\n",
    "    tokenizer = get_chat_template(\n",
    "        tokenizer,\n",
    "        chat_template=\"gemma-3\",\n",
    "    )\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3706aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# DATASET + PROMPTING\n",
    "# =========================\n",
    "\n",
    "\n",
    "def join_docs(docs: List[str]) -> str:\n",
    "    \"\"\"Join top-k docs and truncate to avoid crazy-long contexts.\"\"\"\n",
    "    if not docs:\n",
    "        return \"\"\n",
    "    text = \"\\n\\n\".join(docs[:MAX_DOCS])\n",
    "    return text[:MAX_DOC_CHARS]\n",
    "\n",
    "\n",
    "def make_prompt(question: str, docs: str) -> str:\n",
    "    \"\"\"User prompt: question + retrieved documents.\"\"\"\n",
    "    return (\n",
    "        \"You are a helpful assistant. Use ONLY the provided documents to answer the question.\\n\\n\"\n",
    "        \"QUESTION:\\n\"\n",
    "        f\"{question}\\n\\n\"\n",
    "        \"DOCUMENTS:\\n\"\n",
    "        f\"{docs}\\n\"\n",
    "    )\n",
    "\n",
    "\n",
    "def formatting_single_example(example: Dict[str, Any], tokenizer) -> Dict[str, str]:\n",
    "    question = example[\"question\"]\n",
    "    answer = example[\"answer\"]\n",
    "    docs = join_docs(example[\"documents\"])\n",
    "\n",
    "    user_prompt = make_prompt(question, docs)\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "        {\"role\": \"assistant\", \"content\": answer},\n",
    "    ]\n",
    "\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False,\n",
    "    )\n",
    "\n",
    "    bos = tokenizer.bos_token or \"\"\n",
    "    if bos and text.startswith(bos):\n",
    "        text = text[len(bos) :]\n",
    "\n",
    "    return {\"text\": text}\n",
    "\n",
    "\n",
    "def load_and_prepare_dataset(tokenizer):\n",
    "    print(f\"Loading dataset: {DATASET_NAME}\")\n",
    "    dataset = load_dataset(DATASET_NAME, split=\"train\")\n",
    "\n",
    "    if MAX_TRAIN_SAMPLES is not None:\n",
    "        n = min(MAX_TRAIN_SAMPLES, len(dataset))\n",
    "        print(f\"Subsampling dataset to {n} examples for budget.\")\n",
    "        dataset = dataset.select(range(n))\n",
    "\n",
    "    def _map_fn(batch):\n",
    "        questions = batch[\"question\"]\n",
    "        answers = batch[\"answer\"]\n",
    "        documents = batch[\"documents\"]\n",
    "\n",
    "        texts = []\n",
    "        for q, a, docs in zip(questions, answers, documents):\n",
    "            ex = {\"question\": q, \"answer\": a, \"documents\": docs}\n",
    "            out = formatting_single_example(ex, tokenizer)\n",
    "            texts.append(out[\"text\"])\n",
    "\n",
    "        return {\"text\": texts}\n",
    "\n",
    "    print(\"Formatting dataset into chat-style text...\")\n",
    "    dataset = dataset.map(\n",
    "        _map_fn,\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names,\n",
    "        desc=\"Formatting prompts\",\n",
    "    )\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "031689df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model and tokenizer...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.11.3: Fast Gemma3 patching. Transformers: 4.57.1.\n",
      "   \\\\   /|    NVIDIA L4. Num GPUs = 1. Max memory: 22.034 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Gemma3 does not support SDPA - switching to fast eager.\n",
      "Applying LoRA (QLoRA) configuration...\n",
      "Unsloth: Making `model.base_model.model.model.language_model` require gradients\n",
      "Loading dataset: FreedomIntelligence/RAG-Instruct\n",
      "Formatting dataset into chat-style text...\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_model_and_tokenizer()\n",
    "dataset = load_and_prepare_dataset(tokenizer)\n",
    "\n",
    "# Only use 80% of data for training, rest for eval\n",
    "train_size = int(0.8 * len(dataset))\n",
    "train_dataset = dataset.select(range(train_size))\n",
    "eval_dataset = dataset.select(range(train_size, len(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3eae169e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bfloat16 supported: True\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "bf16 = is_bfloat16_supported()\n",
    "print(f\"bfloat16 supported: {bf16}\")\n",
    "report_to = \"wandb\" if USE_WANDB else \"none\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada31dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCS sync callback enabled.\n",
      "Starting SFT training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 32,432 | Num Epochs = 1 | Total steps = 8,108\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 4 x 1) = 4\n",
      " \"-____-\"     Trainable parameters = 14,901,248 of 4,314,980,720 (0.35% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8108' max='8108' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8108/8108 7:34:44, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.673700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.243800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.217400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.194700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.191400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.165900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.148600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.168800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.145600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.154300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>2.127000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>2.134600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>2.128500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>2.150200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.117400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>2.138900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>2.151600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>2.142200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>2.119300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.134500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>2.108300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>2.114400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>2.113700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>2.106600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.099800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>2.107100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>2.109900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>2.122800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>2.096700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>2.095600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>2.104400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>2.098700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>2.097300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>2.098100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>2.096900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>2.084100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>2.081300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>2.087700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>2.083700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>2.095500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8108, training_loss=2.1408763635199706, metrics={'train_runtime': 27333.9796, 'train_samples_per_second': 1.187, 'train_steps_per_second': 0.297, 'total_flos': 3.1683510571566246e+17, 'train_loss': 2.1408763635199706, 'epoch': 1.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    logging_steps=200,\n",
    "    save_steps=100,\n",
    "    save_total_limit=3,\n",
    "    bf16=bf16,\n",
    "    fp16=not bf16,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    report_to=report_to,\n",
    ")\n",
    "\n",
    "callbacks = []\n",
    "if ENABLE_GCS_SYNC:\n",
    "    gcs_callback = GCSSyncCallback(\n",
    "        local_dir=OUTPUT_DIR,\n",
    "        bucket_name=BUCKET_NAME,\n",
    "        prefix=GCS_PREFIX,\n",
    "    )\n",
    "    callbacks.append(gcs_callback)\n",
    "    print(\"GCS sync callback enabled.\")\n",
    "\n",
    "print(\"Starting SFT training...\")\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    packing=True,\n",
    "    args=training_args,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fa39616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to /home/yashpareek_workmail/unsloth_finetuning/notebooks/rag-instruct-gemma-3-finetuned\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "save_dir = Path(\"rag-instruct-gemma-3-finetuned\")\n",
    "\n",
    "save_dir.mkdir(exist_ok=True)\n",
    "\n",
    "model.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "print(\"Saved to\", save_dir.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbe467b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastModel\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "base_model_name = MODEL_NAME\n",
    "adapter_dir = \"rag-instruct-gemma-3-finetuned\"\n",
    "\n",
    "base_model, base_tokenizer = FastModel.from_pretrained(\n",
    "    base_model_name,\n",
    "    max_seq_length=2048,\n",
    "    load_in_4bit=True,\n",
    "    load_in_8bit=False,\n",
    "    full_finetuning=False,\n",
    ")\n",
    "\n",
    "from peft import PeftModel\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, adapter_dir)\n",
    "tokenizer = base_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b82216a",
   "metadata": {},
   "source": [
    "### To get checkpoint to train again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d917d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_ckpt = get_last_checkpoint(OUTPUT_DIR)\n",
    "if last_ckpt is not None:\n",
    "    print(f\"Resuming from checkpoint: {last_ckpt}\")\n",
    "    trainer.train(resume_from_checkpoint=last_ckpt)\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting fresh.\")\n",
    "    trainer.train()\n",
    "\n",
    "print(\"Saving final adapter + tokenizer...\")\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec791ff",
   "metadata": {},
   "source": [
    "## Evaluate Base vs Fine-tuned Model\n",
    "\n",
    "This section deals with evaluating the performance of a base model against a fine-tuned model using a specific dataset. The evaluation process involves loading both models, running them on the same eval dataset, and comparing their outputs based on predefined metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da2a0beb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"<start_of_turn>user\\nYou are a helpful assistant. Use ONLY the provided documents to answer the question.\\n\\nQUESTION:\\nIdentify a statistical approach that explores the structure of a text based on word occurrence patterns instead of meaning.\\n\\nDOCUMENTS:\\nthe University of SÃ£o Paulo's Institute of Mathematical and Computer Sciences published a paper detailing a study using statistical methods to analyse the relationships of the words in the text. Instead of trying to find the meaning, Amancio's team used complex network modelling to look for connections and clusters of words. By employing concepts such as frequency and intermittence, which measure occurrence and concentration of a term in the text, Amancio was able to discover the manuscript's keywords and create three-dimensional models of the text's structure and word frequencies. Their conclusion was that in 90% of cases, the Voynich systems\\n\\nof interesting, meaningful pieces of content. By systematically labeling the content of a set of texts, researchers can analyse patterns of content quantitatively using statistical methods, or use qualitative methods to analyse meanings of content within texts. Computers are increasingly used in content analysis to automate the labeling (or coding) of documents. Simple computational techniques can provide descriptive data such as word frequencies and document lengths. Machine learning classifiers can greatly increase the number of texts that can be labeled, but the scientific utility of doing so is a matter of debate. Content analysis is best understood as a broad<end_of_turn>\\n<start_of_turn>model\\nA statistical approach that explores the structure of a text based on word occurrence patterns rather than meaning involves using complex network modeling. This method focuses on analyzing the relationships between words in a text using techniques such as frequency and intermittence to discover keywords and create models of the text's structure. By breaking the text into chunks and analyzing the frequency of common words within these chunks, it provides a unique numerical representation of the text in a multi-dimensional space. Principal components analysis (PCA) can then be used to visualize these relationships, helping to identify underlying patterns and connections in the author's style or thematic elements without directly interpreting the content's meaning.<end_of_turn>\\n\"}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset_demo = eval_dataset.shuffle(seed=SEED).select(range(5))  # small eval for demo\n",
    "eval_dataset_demo[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fd37361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:\n",
      " You are a helpful assistant. Use ONLY the provided documents to answer the question.\n",
      "\n",
      "QUESTION:\n",
      "Identify a statistical approach that explores the structure of a text based on word occurrence patterns instead of meaning.\n",
      "\n",
      "DOCUMENTS:\n",
      "the University of SÃ£o Paulo's Institute of Mathematical and Computer Sciences published a paper detailing a study using statistical methods to analyse the relationships of the words in the text. Instead of trying to find the meaning, Amancio's team used complex network modelling to look for connections and clusters of words. By employing concepts such as frequency and intermittence, which measure occurrence and concentration of a term in the text, Amancio was able to discover the manuscript's keywords and create three-dimensional models of the text's structure and word frequencies. Their conclusion was that in 90% of cases, the Voynich systems\n",
      "\n",
      "of interesting, meaningful pieces of content. By systematically labeling the content of a set of texts, researchers can analyse patterns of content quantitatively using statistical methods, or use qualitative methods to analyse meanings of content within texts. Computers are increasingly used in content analysis to automate the labeling (or coding) of documents. Simple computational techniques can provide descriptive data such as word frequencies and document lengths. Machine learning classifiers can greatly increase the number of texts that can be labeled, but the scientific utility of doing so is a matter of debate. Content analysis is best understood as a broad\n",
      "REF:\n",
      " A statistical approach that explores the structure of a text based on word occurrence patterns rather than meaning involves using complex network modeling. This method focuses on analyzing the relationships between words in a text using techniques such as frequency and intermittence to discover keywords and create models of the text's structure. By breaking the text into chunks and analyzing the frequency of common words within these chunks, it provides a unique numerical representation of the text in a multi-dimensional space. Principal components analysis (PCA) can then be used to visualize these relationships, helping to identify underlying patterns and connections in the author's style or thematic elements without directly interpreting the content's meaning.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "USER_BLOCK_RE  = re.compile(r\"<start_of_turn>user\\s*\\n(.*?)<end_of_turn>\", re.DOTALL)\n",
    "MODEL_BLOCK_RE = re.compile(r\"<start_of_turn>model\\s*\\n(.*?)<end_of_turn>\", re.DOTALL)\n",
    "\n",
    "def extract_prompt_and_ref(chat_text: str):\n",
    "    um = USER_BLOCK_RE.search(chat_text)\n",
    "    mm = MODEL_BLOCK_RE.search(chat_text)\n",
    "    if not um or not mm:\n",
    "        raise ValueError(\"Could not parse <start_of_turn> blocks from eval text.\")\n",
    "    user_prompt = um.group(1).strip()\n",
    "    ref_answer  = mm.group(1).strip()\n",
    "    return user_prompt, ref_answer\n",
    "\n",
    "# sanity check on example 0\n",
    "prompt0, ref0 = extract_prompt_and_ref(eval_dataset_demo[\"text\"][0])\n",
    "print(\"PROMPT:\\n\", prompt0)\n",
    "print(\"REF:\\n\", ref0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99be5566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The statistical approach described in the document is complex network modelling. Amancioâ€™s team used this method to look for connections and clusters of words based on their occurrence patterns (frequency and intermittence) rather than focusing on the meaning of the words.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inferencing with base model\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"gemma-3\",\n",
    ")\n",
    "model.to(\"cuda\")\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"\"\"You are a helpful assistant. \n",
    "        Use ONLY the provided documents to answer the question.\n",
    "        \\n\\nQUESTION:\\nIdentify a statistical approach that explores the structure of a text based on word occurrence patterns instead of meaning.\n",
    "        \\n\\nDOCUMENTS:\\nthe University of SÃ£o Paulo's Institute of Mathematical and Computer Sciences published a paper detailing a study using statistical methods to analyse the relationships of the words in the text. Instead of trying to find the meaning, Amancio's team used complex network modelling to look for connections and clusters of words. By employing concepts such as frequency and intermittence, which measure occurrence and concentration of a term in the text, Amancio was able to discover the manuscript's keywords and create three-dimensional models of the text's structure and word frequencies. Their conclusion was that in 90% of cases, the Voynich systems \n",
    "        of interesting, meaningful pieces of content. By systematically labeling the content of a set of texts, researchers can analyse patterns of content quantitatively using statistical methods, or use qualitative methods to analyse meanings of content within texts. Computers are increasingly used in content analysis to automate the labeling (or coding) of documents. Simple computational techniques can provide descriptive data such as word frequencies and document lengths. Machine learning classifiers can greatly increase the number of texts that can be labeled, but the scientific utility of doing so is a matter of debate. Content analysis is best understood as a broad\"\"\",\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,  # Must add for generation\n",
    "    tokenize=True,\n",
    "    return_tensors=\"pt\",\n",
    "    return_dict=True,\n",
    ")\n",
    "outputs = model.generate(\n",
    "    **inputs.to(\"cuda\"),\n",
    "    max_new_tokens=64,\n",
    "    temperature=0.1,\n",
    "    top_p=0,\n",
    "    top_k=64,\n",
    ")\n",
    "tokenizer.batch_decode(outputs)\n",
    "\n",
    "re.search(\n",
    "    r\"<start_of_turn>model\\s*\\n(.*?)<end_of_turn>\",\n",
    "    tokenizer.batch_decode(outputs)[0],\n",
    "    re.DOTALL,\n",
    ").group(1).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdce3a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yashpareek_workmail/unsloth-venv/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yashpareek_workmail/unsloth-venv/lib/python3.12/site-packages/peft/peft_model.py:598: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.base_model.model.model.language_model.layers.0.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.0.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.0.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.0.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.0.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.0.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.0.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.0.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.0.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.0.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.0.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.0.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.0.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.0.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.1.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.1.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.1.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.1.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.1.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.1.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.1.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.1.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.1.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.1.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.1.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.1.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.1.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.1.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.2.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.2.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.2.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.2.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.2.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.2.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.2.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.2.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.2.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.2.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.2.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.2.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.2.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.2.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.3.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.3.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.3.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.3.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.3.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.3.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.3.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.3.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.3.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.3.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.3.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.3.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.3.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.3.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.4.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.4.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.4.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.4.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.4.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.4.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.4.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.4.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.4.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.4.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.4.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.4.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.4.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.4.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.5.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.5.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.5.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.5.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.5.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.5.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.5.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.5.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.5.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.5.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.5.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.5.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.5.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.5.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.6.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.6.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.6.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.6.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.6.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.6.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.6.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.6.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.6.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.6.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.6.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.6.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.6.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.6.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.7.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.7.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.7.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.7.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.7.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.7.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.7.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.7.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.7.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.7.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.7.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.7.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.7.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.7.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.8.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.8.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.8.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.8.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.8.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.8.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.8.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.8.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.8.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.8.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.8.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.8.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.8.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.8.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.9.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.9.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.9.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.9.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.9.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.9.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.9.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.9.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.9.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.9.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.9.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.9.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.9.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.9.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.10.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.10.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.10.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.10.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.10.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.10.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.10.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.10.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.10.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.10.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.10.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.10.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.10.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.10.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.11.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.11.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.11.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.11.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.11.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.11.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.11.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.11.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.11.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.11.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.11.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.11.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.11.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.11.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.12.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.12.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.12.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.12.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.12.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.12.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.12.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.12.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.12.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.12.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.12.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.12.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.12.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.12.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.13.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.13.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.13.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.13.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.13.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.13.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.13.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.13.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.13.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.13.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.13.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.13.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.13.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.13.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.14.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.14.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.14.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.14.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.14.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.14.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.14.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.14.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.14.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.14.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.14.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.14.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.14.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.14.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.15.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.15.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.15.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.15.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.15.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.15.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.15.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.15.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.15.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.15.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.15.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.15.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.15.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.15.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.16.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.16.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.16.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.16.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.16.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.16.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.16.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.16.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.16.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.16.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.16.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.16.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.16.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.16.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.17.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.17.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.17.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.17.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.17.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.17.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.17.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.17.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.17.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.17.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.17.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.17.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.17.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.17.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.18.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.18.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.18.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.18.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.18.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.18.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.18.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.18.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.18.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.18.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.18.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.18.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.18.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.18.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.19.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.19.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.19.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.19.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.19.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.19.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.19.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.19.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.19.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.19.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.19.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.19.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.19.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.19.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.20.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.20.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.20.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.20.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.20.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.20.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.20.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.20.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.20.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.20.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.20.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.20.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.20.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.20.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.21.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.21.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.21.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.21.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.21.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.21.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.21.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.21.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.21.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.21.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.21.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.21.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.21.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.21.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.22.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.22.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.22.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.22.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.22.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.22.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.22.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.22.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.22.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.22.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.22.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.22.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.22.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.22.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.23.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.23.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.23.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.23.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.23.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.23.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.23.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.23.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.23.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.23.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.23.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.23.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.23.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.23.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.24.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.24.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.24.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.24.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.24.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.24.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.24.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.24.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.24.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.24.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.24.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.24.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.24.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.24.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.25.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.25.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.25.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.25.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.25.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.25.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.25.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.25.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.25.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.25.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.25.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.25.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.25.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.25.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.26.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.26.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.26.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.26.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.26.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.26.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.26.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.26.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.26.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.26.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.26.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.26.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.26.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.26.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.27.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.27.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.27.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.27.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.27.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.27.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.27.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.27.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.27.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.27.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.27.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.27.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.27.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.27.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.28.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.28.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.28.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.28.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.28.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.28.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.28.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.28.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.28.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.28.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.28.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.28.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.28.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.28.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.29.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.29.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.29.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.29.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.29.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.29.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.29.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.29.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.29.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.29.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.29.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.29.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.29.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.29.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.30.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.30.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.30.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.30.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.30.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.30.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.30.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.30.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.30.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.30.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.30.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.30.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.30.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.30.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.31.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.31.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.31.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.31.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.31.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.31.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.31.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.31.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.31.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.31.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.31.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.31.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.31.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.31.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.32.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.32.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.32.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.32.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.32.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.32.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.32.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.32.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.32.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.32.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.32.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.32.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.32.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.32.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.33.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.33.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.33.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.33.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.33.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.33.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.33.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.33.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.33.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.33.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.33.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.33.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.language_model.layers.33.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.language_model.layers.33.mlp.down_proj.lora_B.default.weight'].\n",
      "  warnings.warn(warn_message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The statistical approach described in the document is complex network modelling. Amancioâ€™s team used this method to look for connections and clusters of words based on their occurrence patterns (frequency and intermittence) rather than focusing on the meaning of the words.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inferencing with adapted model\n",
    "from peft import PeftModel\n",
    "\n",
    "ADAPTER_DIR = \"rag-instruct-gemma-3-finetuned\"\n",
    "adapted_model = PeftModel.from_pretrained(model, ADAPTER_DIR)\n",
    "adapted_outputs = adapted_model.generate(\n",
    "    **inputs.to(\"cuda\"),\n",
    "    max_new_tokens=64,\n",
    "    temperature=0.1,\n",
    "    top_p=0,\n",
    "    top_k=64,\n",
    ")\n",
    "tokenizer.batch_decode(adapted_outputs)\n",
    "\n",
    "re.search(\n",
    "    r\"<start_of_turn>model\\s*\\n(.*?)<end_of_turn>\",\n",
    "    tokenizer.batch_decode(adapted_outputs)[0],\n",
    "    re.DOTALL,\n",
    ").group(1).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c92e790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.11.3: Fast Gemma3 patching. Transformers: 4.57.1.\n",
      "   \\\\   /|    NVIDIA L4. Num GPUs = 1. Max memory: 22.034 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Gemma3 does not support SDPA - switching to fast eager.\n",
      "The statistical approach that explores the structure of a text based on word occurrence patterns is a complex network modelling using concepts like frequency and intermittence. This approach aims to uncover connections and clusters of words in a text by analyzing patterns of word occurrence without focusing on the meaning of the words. For instance, in the Voynich manuscript, Amancio's team used such methods to create three-dimensional models of the text's structure and word frequencies. This technique is notable because it doesn't attempt to\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    from unsloth import FastModel\n",
    "\n",
    "    model, tokenizer = FastModel.from_pretrained(\n",
    "        model_name=ADAPTER_DIR,\n",
    "        max_seq_length=2048,\n",
    "        load_in_4bit=True,\n",
    "    )\n",
    "\n",
    "model.to(\"cuda\")\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_tensors=\"pt\",\n",
    "    return_dict=True,\n",
    ")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "\n",
    "_ = model.generate(\n",
    "    **inputs.to(\"cuda\"),\n",
    "    max_new_tokens=100,\n",
    "    temperature=1.0,\n",
    "    top_p=0.95,\n",
    "    top_k=64,\n",
    "    streamer=TextStreamer(tokenizer, skip_prompt=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b6b552",
   "metadata": {},
   "source": [
    "### Load and inference with both models\n",
    "Try running inference with both models on the same input to see the difference in outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e3ab64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.11.3: Fast Gemma3 patching. Transformers: 4.57.1.\n",
      "   \\\\   /|    NVIDIA L4. Num GPUs = 1. Max memory: 22.034 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Gemma3 does not support SDPA - switching to fast eager.\n",
      "==((====))==  Unsloth 2025.11.3: Fast Gemma3 patching. Transformers: 4.57.1.\n",
      "   \\\\   /|    NVIDIA L4. Num GPUs = 1. Max memory: 22.034 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Gemma3 does not support SDPA - switching to fast eager.\n",
      "\n",
      "================ BASE ANSWER ================\n",
      " The provided document describes a study using â€œcomplex network modellingâ€ to explore the structure of a text based on â€œword occurrence patternsâ€ â€“ specifically, using concepts like â€œfrequency and intermittenceâ€ (occurrence and concentration of a term).\n",
      "\n",
      "=============== ADAPTER ANSWER ==============\n",
      " One statistical approach used to explore the structure of a text based on word occurrence patterns is complex network modelling. Researchers can analyze patterns and clusters of words by using statistical methods like frequency and intermittence to measure occurrence and concentration of terms in the text. By identifying keywords and creating three-dimensional models of the text's structure and word frequencies, the analysis reveals insights into the thematic or conceptual organization of the text, without necessarily focusing on the explicit meaning.\n",
      "\n",
      "Changed? True\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastModel\n",
    "from transformers import TextStreamer\n",
    "\n",
    "# ---- LOAD BASE (no adapter) ----\n",
    "base_model, base_tokenizer = FastModel.from_pretrained(\n",
    "    model_name=MODEL_NAME,\n",
    "    max_seq_length=2048,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "# ---- LOAD ADAPTED (adapter dir) ----\n",
    "adapt_model, adapt_tokenizer = FastModel.from_pretrained(\n",
    "    model_name=ADAPTER_DIR,\n",
    "    max_seq_length=2048,\n",
    "    load_in_4bit=True,\n",
    "    load_in_16bit=False,\n",
    ")\n",
    "\n",
    "\n",
    "# ---- helper: generate output text (same apply_chat_template style) ----\n",
    "def generate_text(model, tokenizer, messages, max_new_tokens=100):\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_tensors=\"pt\",\n",
    "        return_dict=True,\n",
    "    )\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs.to(\"cuda\"),\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=1.0,\n",
    "        top_p=0.95,\n",
    "        top_k=64,\n",
    "    )\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=False)[0]\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"\"\"You are a helpful assistant. \n",
    "        Use ONLY the provided documents to answer the question.\n",
    "        \\n\\nQUESTION:\\nIdentify a statistical approach that explores the structure of a text based on word occurrence patterns instead of meaning.\n",
    "        \\n\\nDOCUMENTS:\\nthe University of SÃ£o Paulo's Institute of Mathematical and Computer Sciences published a paper detailing a study using statistical methods to analyse the relationships of the words in the text. Instead of trying to find the meaning, Amancio's team used complex network modelling to look for connections and clusters of words. By employing concepts such as frequency and intermittence, which measure occurrence and concentration of a term in the text, Amancio was able to discover the manuscript's keywords and create three-dimensional models of the text's structure and word frequencies. Their conclusion was that in 90% of cases, the Voynich systems \n",
    "        of interesting, meaningful pieces of content. By systematically labeling the content of a set of texts, researchers can analyse patterns of content quantitatively using statistical methods, or use qualitative methods to analyse meanings of content within texts. Computers are increasingly used in content analysis to automate the labeling (or coding) of documents. Simple computational techniques can provide descriptive data such as word frequencies and document lengths. Machine learning classifiers can greatly increase the number of texts that can be labeled, but the scientific utility of doing so is a matter of debate. Content analysis is best understood as a broad\"\"\",\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "base_decoded = generate_text(base_model, base_tokenizer, messages, max_new_tokens=100)\n",
    "adapt_decoded = generate_text(\n",
    "    adapt_model, adapt_tokenizer, messages, max_new_tokens=100\n",
    ")\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "def extract_answer(decoded):\n",
    "    return (\n",
    "        re.search(r\"<start_of_turn>model\\s*\\n(.*?)<end_of_turn>\", decoded, re.DOTALL)\n",
    "        .group(1)\n",
    "        .strip()\n",
    "    )\n",
    "\n",
    "\n",
    "base_answer = extract_answer(base_decoded)\n",
    "adapt_answer = extract_answer(adapt_decoded)\n",
    "\n",
    "print(\"\\n================ BASE ANSWER ================\\n\", base_answer)\n",
    "print(\"\\n=============== ADAPTER ANSWER ==============\\n\", adapt_answer)\n",
    "print(\"\\nChanged?\", base_answer != adapt_answer)\n",
    "\n",
    "del base_model\n",
    "del adapt_model\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "\n",
    "def clear_gpu():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "\n",
    "\n",
    "clear_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682c3502",
   "metadata": {},
   "source": [
    "### Try first 5 examples from eval dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac275bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.11.3: Fast Gemma3 patching. Transformers: 4.57.1.\n",
      "   \\\\   /|    NVIDIA L4. Num GPUs = 1. Max memory: 22.034 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Gemma3 does not support SDPA - switching to fast eager.\n",
      "==((====))==  Unsloth 2025.11.3: Fast Gemma3 patching. Transformers: 4.57.1.\n",
      "   \\\\   /|    NVIDIA L4. Num GPUs = 1. Max memory: 22.034 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Gemma3 does not support SDPA - switching to fast eager.\n",
      "\n",
      "=== SUMMARY (5 examples) ===\n",
      "EM   base: 0.0  adapted: 0.0\n",
      "RL-F1 base: 0.1545488548104122  adapted: 0.21684093978040136\n",
      "\n",
      "--- EX 0 ---\n",
      "REF:\n",
      " A statistical approach that explores the structure of a text based on word occurrence patterns rather than meaning involves using complex network modeling. This method focuses on analyzing the relationships between words in a text using techniques such as frequency and intermittence to discover keywords and create models of the text's structure. By breaking the text into chunks and analyzing the frequency of common words within these chunks, it provides a unique numerical representation of the text in a multi-dimensional space. Principal components analysis (PCA) can then be used to visualize these relationships, helping to identify underlying patterns and connections in the author's style or thematic elements without directly interpreting the content's meaning.\n",
      "\n",
      "BASE:\n",
      " The statistical approach described in the document is complex network modelling. It uses concepts like frequency and intermittence (occurrence and concentration of a term) to explore the relationships and clusters of words in a text, rather than focusing on the meaning of the words themselves.\n",
      "\n",
      "ADAPTED:\n",
      " One statistical approach that explores the structure of a text based on word occurrence patterns without focusing on meaning is the method used by Amancio's team at the University of SÃ£o Paulo's Institute of Mathematical and Computer Sciences. They utilized complex network modelling to analyze relationships and clusters of words, employing concepts like frequency and intermittence to discover keywords and create three-dimensional models of the text's structure and word frequencies.\n",
      "\n",
      "Scores: EM base=0.0 adpt=0.0 | ROUGE-L base=0.228 adpt=0.339\n",
      "\n",
      "--- EX 1 ---\n",
      "REF:\n",
      " The film's set that Lana Del Rey's 2012 cover version of 'Blue Velvet' is connected with is David Lynch's 'Blue Velvet'. The connection is established because Peter Braatz, who was invited by David Lynch in 1985, shot footage on the set of 'Blue Velvet'. This footage eventually contributed to the creation of a documentary film revisiting 'Blue Velvet'. Lana Del Reyâ€™s cover of 'Blue Velvet' in 2012, hence, links back to the original film through the inclusion and significance of this originally shot footage.\n",
      "\n",
      "BASE:\n",
      " The filmâ€™s set that had a connection due to the origins of its behind-the-scenes footage was the one that was originally a school in Rome where Roberto Rossellini attended.\n",
      "\n",
      "ADAPTED:\n",
      " The film that had a connection with Lana Del Rey's 2012 cover of 'Blue Velvet' due to the origins of its behind-the-scenes footage was \"Blue Velvet.\" The behind-the-scenes footage of \"Blue Velvet\" was originally shown on television, and this has a connection with the musical television performance \"Live In Concert\" by Lana Del Rey in July 2012, in which the cover version was performed live.\n",
      "\n",
      "Scores: EM base=0.0 adpt=0.0 | ROUGE-L base=0.177 adpt=0.282\n",
      "\n",
      "--- EX 2 ---\n",
      "REF:\n",
      " In Python 3.0, disparate types, such as a codice_95 and a codice_96, which were historically defined to have a consistent relative ordering, will no longer be compared this way.\n",
      "\n",
      "BASE:\n",
      " The provided text does not mention any change regarding type comparisons in Python 3.0. It discusses comparisons in C++, Perl, PHP, Ruby, and Apache Groovy using the \"spaceship operator.\"\n",
      "\n",
      "ADAPTED:\n",
      " In Python 3.0, a change is noted regarding the behavior of chained comparisons by introducing short-circuit semantics. This means that comparisons will be evaluated from left to right, stopping as soon as a result is clear. This contrasts with earlier versions of Python where more complex conditions could lead to issues and errors.\n",
      "\n",
      "Scores: EM base=0.0 adpt=0.0 | ROUGE-L base=0.103 adpt=0.171\n",
      "\n",
      "--- EX 3 ---\n",
      "REF:\n",
      " Soil moisture is typically measured using various techniques such as time-domain reflectometry (TDR), frequency-domain sensors, neutron probes, tensiometers, and gravimetric methods. Each technique has its advantages and is selected based on factors like accuracy, soil type, depth, and environmental conditions. TDR and frequency-domain sensors are widely used for their precision and ability to provide continuous data, while gravimetric methods are considered standard for validation due to their direct measurement of water content by weight.\n",
      "\n",
      "BASE:\n",
      " The provided documents do not contain information about soil moisture measurement techniques. They describe a park's biodiversity and habitat types.\n",
      "\n",
      "ADAPTED:\n",
      " The most common soil moisture measurements used in climate research include the use of thaw-a-tapes or other soil temperature probes that automatically measure soil temperature and water content to provide daily soil moisture data, as well as more sophisticated methods like the use of satellite products like the Climate Prediction Centre's Soil Moisture Machine. These techniques provide detailed and frequent measurements of soil moisture, critical for understanding soil-vegetation relationships and predicting climate trends.\n",
      "\n",
      "Scores: EM base=0.0 adpt=0.0 | ROUGE-L base=0.064 adpt=0.122\n",
      "\n",
      "--- EX 4 ---\n",
      "REF:\n",
      " According to the document, in 2009, Israeli authorities reported the removal of 27 checkpoints and 140 roadblocks in the West Bank as part of an effort to ease security restrictions (see Ref. [1]). This removal likely had a positive impact on Palestinian mobility by reducing some of the physical barriers that impeded movement within the West Bank.\n",
      "\n",
      "However, the broader context provided by the World Bank's 2007 report and other references in the document paints a more complex picture. The World Bank report described the situation in the West Bank as being heavily influenced by a comprehensive network of policies, practices, and physical impediments that fragmented the territory into smaller, disconnected segments (see Ref. [10]). Even with the removal of some roadblocks and checkpoints in 2009, other significant restrictions remained. For instance, numerous roadblocks, trenches, earth mounds, and various types of barriers still impeded Palestinian mobility (see Refs. [2],[5],[6]).\n",
      "\n",
      "Moreover, while there was a reduction in the number of checkpoints, the system of control and restriction of Palestinian movement continued through other means such as the construction of the separation barrier and the maintenance of a complex permit system (see Refs. [2],[6]). Additionally, in 2012, there were still 99 fixed checkpoints and 310 flying checkpoints restricting movement (see Ref. [6]). \n",
      "\n",
      "Therefore, while the removal of certain roadblocks and checkpoints in 2009 may have provided some relief, it was likely insufficient to significantly freedom of movement for Palestinians, given the extensive and multifaceted nature of the remaining restrictions described in the document.\n",
      "\n",
      "BASE:\n",
      " According to the document, in July 2009, Israeli authorities reported removing 27 checkpoints and 140 roadblocks in the West Bank. This action was intended to ease security restrictions. However, the document also details that despite these removals, there are still 50 block sites and 275 earth mounds, along with 10 trenches and 37,600 meters of fences, impeding Palestinian travel. This suggests that while some physical obstacles were reduced,\n",
      "\n",
      "ADAPTED:\n",
      " The removal of roadblocks and checkpoints by Israeli authorities in 2009, as stated in the 2009 report, likely facilitated increased mobility for Palestinians. These restrictions significantly hampered daily life, education, employment, and personal freedoms. The World Bank's 2007 report highlighted that Palestinian mobility was curtailed to 14.2% of the population, and the removal of checkpoints was cited as a means to alleviate this issue. In 2009,\n",
      "\n",
      "Scores: EM base=0.0 adpt=0.0 | ROUGE-L base=0.201 adpt=0.170\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastModel\n",
    "from transformers import TextStreamer\n",
    "\n",
    "# ---- LOAD BASE (no adapter) ----\n",
    "base_model, base_tokenizer = FastModel.from_pretrained(\n",
    "    model_name=MODEL_NAME,\n",
    "    max_seq_length=2048,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "# ---- LOAD ADAPTED (adapter dir) ----\n",
    "adapt_model, adapt_tokenizer = FastModel.from_pretrained(\n",
    "    model_name=ADAPTER_DIR,\n",
    "    max_seq_length=2048,\n",
    "    load_in_4bit=True,\n",
    "    load_in_16bit=False,\n",
    ")\n",
    "\n",
    "\n",
    "## Metrics\n",
    "def _normalize(s: str) -> str:\n",
    "    return \" \".join(s.lower().strip().split())\n",
    "\n",
    "\n",
    "def exact_match(pred: str, ref: str) -> float:\n",
    "    return float(_normalize(pred) == _normalize(ref))\n",
    "\n",
    "\n",
    "def _lcs_len(a, b):\n",
    "    n, m = len(a), len(b)\n",
    "    dp = [0] * (m + 1)\n",
    "    for i in range(1, n + 1):\n",
    "        prev = 0\n",
    "        for j in range(1, m + 1):\n",
    "            tmp = dp[j]\n",
    "            if a[i - 1] == b[j - 1]:\n",
    "                dp[j] = prev + 1\n",
    "            else:\n",
    "                dp[j] = max(dp[j], dp[j - 1])\n",
    "            prev = tmp\n",
    "    return dp[m]\n",
    "\n",
    "\n",
    "def rouge_l_f1(pred: str, ref: str) -> float:\n",
    "    p = _normalize(pred).split()\n",
    "    r = _normalize(ref).split()\n",
    "    if not p or not r:\n",
    "        return 0.0\n",
    "    lcs = _lcs_len(p, r)\n",
    "    prec = lcs / len(p)\n",
    "    rec = lcs / len(r)\n",
    "    return (2 * prec * rec / (prec + rec)) if (prec + rec) else 0.0\n",
    "\n",
    "\n",
    "# Parsing Answer\n",
    "def extract_answer(decoded):\n",
    "    m = re.search(r\"<start_of_turn>model\\s*\\n(.*?)<end_of_turn>\", decoded, re.DOTALL)\n",
    "    if m:\n",
    "        return m.group(1).strip()\n",
    "\n",
    "    # Fallback 1: if model start exists but no end_of_turn (truncated)\n",
    "    m2 = re.search(r\"<start_of_turn>model\\s*\\n(.*)$\", decoded, re.DOTALL)\n",
    "    if m2:\n",
    "        return m2.group(1).strip()\n",
    "\n",
    "    # Fallback 2: no tags at all -> return decoded as-is\n",
    "    return decoded.strip()\n",
    "\n",
    "\n",
    "def generate_text(model, tokenizer, messages, max_new_tokens=100):\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_tensors=\"pt\",\n",
    "        return_dict=True,\n",
    "    )\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs.to(\"cuda\"),\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=1.0,\n",
    "        top_p=0.95,\n",
    "        top_k=64,\n",
    "    )\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=False)[0]\n",
    "\n",
    "\n",
    "results = []\n",
    "for i in range(5):\n",
    "    chat_text = eval_dataset_demo[\"text\"][i]\n",
    "    user_prompt, ref = extract_prompt_and_ref(chat_text)\n",
    "\n",
    "    # build messages exactly like you do (multimodal text wrapper)\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": user_prompt,\n",
    "                }\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    base_decoded = generate_text(\n",
    "        base_model, base_tokenizer, messages, max_new_tokens=100\n",
    "    )\n",
    "    adapt_decoded = generate_text(\n",
    "        adapt_model, adapt_tokenizer, messages, max_new_tokens=100\n",
    "    )\n",
    "\n",
    "    pred_base = extract_answer(base_decoded)\n",
    "    pred_adpt = extract_answer(adapt_decoded)\n",
    "\n",
    "    results.append(\n",
    "        {\n",
    "            \"i\": i,\n",
    "            \"em_base\": exact_match(pred_base, ref),\n",
    "            \"em_adpt\": exact_match(pred_adpt, ref),\n",
    "            \"rl_base\": rouge_l_f1(pred_base, ref),\n",
    "            \"rl_adpt\": rouge_l_f1(pred_adpt, ref),\n",
    "            \"ref\": ref,\n",
    "            \"base\": pred_base,\n",
    "            \"adapted\": pred_adpt,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# ======================\n",
    "# PRINT SUMMARY + DETAILS\n",
    "# ======================\n",
    "print(\"\\n=== SUMMARY (5 examples) ===\")\n",
    "print(\n",
    "    \"EM   base:\",\n",
    "    sum(r[\"em_base\"] for r in results) / len(results),\n",
    "    \" adapted:\",\n",
    "    sum(r[\"em_adpt\"] for r in results) / len(results),\n",
    ")\n",
    "print(\n",
    "    \"RL-F1 base:\",\n",
    "    sum(r[\"rl_base\"] for r in results) / len(results),\n",
    "    \" adapted:\",\n",
    "    sum(r[\"rl_adpt\"] for r in results) / len(results),\n",
    ")\n",
    "\n",
    "for r in results:\n",
    "    print(f\"\\n--- EX {r['i']} ---\")\n",
    "    print(\"REF:\\n\", r[\"ref\"])\n",
    "    print(\"\\nBASE:\\n\", r[\"base\"])\n",
    "    print(\"\\nADAPTED:\\n\", r[\"adapted\"])\n",
    "    print(\n",
    "        f\"\\nScores: EM base={r['em_base']} adpt={r['em_adpt']} | ROUGE-L base={r['rl_base']:.3f} adpt={r['rl_adpt']:.3f}\"\n",
    "    )\n",
    "\n",
    "# ======================\n",
    "# CLEANUP GPU (your style)\n",
    "# ======================\n",
    "del base_model\n",
    "del adapt_model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99a035d",
   "metadata": {},
   "source": [
    "ROUGE-L F1 improved from 0.155 â†’ 0.217 (â‰ˆ +0.062 absolute, ~+40% relative). That means the adapted modelâ€™s answers share more subsequence overlap with the reference answers overall."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth-venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
