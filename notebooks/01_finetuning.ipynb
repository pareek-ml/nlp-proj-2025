{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1670fe17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yashpareek_workmail/unsloth-venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_2105/340567520.py:19: UserWarning: WARNING: Unsloth should be imported before transformers to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import FastModel, is_bfloat16_supported\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpareek-ml\u001b[0m (\u001b[33mpareek-ml-personal\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/yashpareek_workmail/unsloth_finetuning/notebooks/wandb/run-20251128_134447-ej1neew6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/pareek-ml-personal/instruct-rag-finetune/runs/ej1neew6' target=\"_blank\">snowy-breeze-2</a></strong> to <a href='https://wandb.ai/pareek-ml-personal/instruct-rag-finetune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/pareek-ml-personal/instruct-rag-finetune' target=\"_blank\">https://wandb.ai/pareek-ml-personal/instruct-rag-finetune</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/pareek-ml-personal/instruct-rag-finetune/runs/ej1neew6' target=\"_blank\">https://wandb.ai/pareek-ml-personal/instruct-rag-finetune/runs/ej1neew6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n"
     ]
    }
   ],
   "source": [
    "# train_rag_unsloth.py\n",
    "import os\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    TrainingArguments,\n",
    "    TrainerCallback,\n",
    "    TrainerState,\n",
    "    TrainerControl,\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "import numpy as np\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "from unsloth import FastModel, is_bfloat16_supported\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "SEED = 3407\n",
    "# set random seed for reproducibility\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "MODEL_NAME = \"unsloth/gemma-3-4b-it\"  # Base model\n",
    "DATASET_NAME = \"FreedomIntelligence/RAG-Instruct\"  # HF dataset\n",
    "OUTPUT_DIR = \"./outputs/gemma3-4b-rag\"\n",
    "\n",
    "BUCKET_NAME = \"model-finetune-1\"  # GCP bucket for finetune checkpoints\n",
    "GCS_PREFIX = \"checkpoints/gemma3-4b-rag\"  # optional path inside bucket\n",
    "ENABLE_GCS_SYNC = True\n",
    "\n",
    "MAX_SEQ_LENGTH = 2048  # safe for 4B on L4 with packing\n",
    "MAX_DOCS = 2  # number of documents per sample\n",
    "MAX_DOC_CHARS = 4000  # truncate context to avoid huge prompts\n",
    "\n",
    "# Budget controls â€“ tune these first\n",
    "MAX_TRAIN_SAMPLES = None  # set to None for full dataset\n",
    "NUM_TRAIN_EPOCHS = 1\n",
    "LEARNING_RATE = 2e-4\n",
    "\n",
    "BATCH_SIZE = 1  # per-device batch size\n",
    "GRAD_ACCUM_STEPS = 4  # effective batch size = BATCH_SIZE * GRAD_ACCUM_STEPS\n",
    "\n",
    "USE_WANDB = True  # set True and export WANDB_PROJECT if you want logging\n",
    "WANDB_PROJECT = \"instruct-rag-finetune\"\n",
    "\n",
    "\n",
    "if USE_WANDB:\n",
    "    run = wandb.init(\n",
    "        # Set the wandb entity where your project will be logged (generally your team name).\n",
    "        entity=\"pareek-ml-personal\",\n",
    "        # Set the wandb project where this run will be logged.\n",
    "        project=\"instruct-rag-finetune\",\n",
    "        # Track hyperparameters and run metadata.\n",
    "        config={\n",
    "            \"model_name\": MODEL_NAME,\n",
    "            \"dataset_name\": DATASET_NAME,\n",
    "            \"max_train_samples\": MAX_TRAIN_SAMPLES,\n",
    "            \"num_train_epochs\": NUM_TRAIN_EPOCHS,\n",
    "            \"learning_rate\": LEARNING_RATE,\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"grad_accum_steps\": GRAD_ACCUM_STEPS,\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "146a5855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# GCS SYNC CALLBACK\n",
    "# =========================\n",
    "\n",
    "class GCSSyncCallback(TrainerCallback):\n",
    "    def __init__(self, local_dir: str, bucket_name: str, prefix: str):\n",
    "        self.local_dir = os.path.abspath(local_dir)\n",
    "        self.bucket_name = bucket_name\n",
    "        self.prefix = prefix.rstrip(\"/\") if prefix else \"\"\n",
    "        self.client = storage.Client()\n",
    "        self.bucket = self.client.bucket(bucket_name)\n",
    "\n",
    "    def on_save(\n",
    "        self,\n",
    "        args: TrainingArguments,\n",
    "        state: TrainerState,\n",
    "        control: TrainerControl,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # Sync only the checkpoint that was just written\n",
    "        if not os.path.exists(self.local_dir):\n",
    "            return\n",
    "\n",
    "        print(f\"[GCSSync] Syncing {self.local_dir} to gs://{self.bucket_name}/{self.prefix} ...\")\n",
    "        self._sync_directory(self.local_dir, self.prefix)\n",
    "        print(\"[GCSSync] Sync complete.\")\n",
    "        return control\n",
    "\n",
    "    def _sync_directory(self, local_dir: str, gcs_prefix: str):\n",
    "        for root, dirs, files in os.walk(local_dir):\n",
    "            for fname in files:\n",
    "                local_path = os.path.join(root, fname)\n",
    "                rel_path = os.path.relpath(local_path, local_dir)\n",
    "                blob_name = f\"{gcs_prefix}/{rel_path}\" if gcs_prefix else rel_path\n",
    "                blob = self.bucket.blob(blob_name)\n",
    "                blob.upload_from_filename(local_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63ee2637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# MODEL & TOKENIZER\n",
    "# =========================\n",
    "\n",
    "def load_model_and_tokenizer():\n",
    "    # If you need HF token for the model, set: os.environ[\"HF_TOKEN\"] = \"hf_xxx\"\n",
    "    print(\"Loading base model and tokenizer...\")\n",
    "    model, tokenizer = FastModel.from_pretrained(\n",
    "        model_name=MODEL_NAME,\n",
    "        max_seq_length=MAX_SEQ_LENGTH,\n",
    "        load_in_4bit=True,\n",
    "        load_in_8bit=False,\n",
    "        full_finetuning=False,\n",
    "    )\n",
    "\n",
    "    print(\"Applying LoRA (QLoRA) configuration...\")\n",
    "    model = FastModel.get_peft_model(\n",
    "        model,\n",
    "        finetune_vision_layers=False,\n",
    "        finetune_language_layers=True,\n",
    "        finetune_attention_modules=True,\n",
    "        finetune_mlp_modules=True,\n",
    "        r=8,            # LoRA rank\n",
    "        lora_alpha=8,   # usually >= r\n",
    "        lora_dropout=0,\n",
    "        bias=\"none\",\n",
    "        random_state=3407,\n",
    "    )\n",
    "\n",
    "    # Prepare for training (gradient checkpointing etc.)\n",
    "    FastModel.for_training(model)\n",
    "\n",
    "    # Attach Gemma-3 chat template\n",
    "    tokenizer = get_chat_template(\n",
    "        tokenizer,\n",
    "        chat_template=\"gemma-3\",\n",
    "    )\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae3706aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# DATASET + PROMPTING\n",
    "# =========================\n",
    "\n",
    "def join_docs(docs: List[str]) -> str:\n",
    "    \"\"\"Join top-k docs and truncate to avoid crazy-long contexts.\"\"\"\n",
    "    if not docs:\n",
    "        return \"\"\n",
    "    text = \"\\n\\n\".join(docs[:MAX_DOCS])\n",
    "    return text[:MAX_DOC_CHARS]\n",
    "\n",
    "\n",
    "def make_prompt(question: str, docs: str) -> str:\n",
    "    \"\"\"User prompt: question + retrieved documents.\"\"\"\n",
    "    return (\n",
    "        \"You are a helpful assistant. Use ONLY the provided documents to answer the question.\\n\\n\"\n",
    "        \"QUESTION:\\n\"\n",
    "        f\"{question}\\n\\n\"\n",
    "        \"DOCUMENTS:\\n\"\n",
    "        f\"{docs}\\n\"\n",
    "    )\n",
    "\n",
    "\n",
    "def formatting_single_example(example: Dict[str, Any], tokenizer) -> Dict[str, str]:\n",
    "    question = example[\"question\"]\n",
    "    answer = example[\"answer\"]\n",
    "    docs = join_docs(example[\"documents\"])\n",
    "\n",
    "    user_prompt = make_prompt(question, docs)\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "        {\"role\": \"assistant\", \"content\": answer},\n",
    "    ]\n",
    "\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False,\n",
    "    )\n",
    "\n",
    "    bos = tokenizer.bos_token or \"\"\n",
    "    if bos and text.startswith(bos):\n",
    "        text = text[len(bos):]\n",
    "\n",
    "    return {\"text\": text}\n",
    "\n",
    "\n",
    "def load_and_prepare_dataset(tokenizer):\n",
    "    print(f\"Loading dataset: {DATASET_NAME}\")\n",
    "    dataset = load_dataset(DATASET_NAME, split=\"train\")\n",
    "\n",
    "    if MAX_TRAIN_SAMPLES is not None:\n",
    "        n = min(MAX_TRAIN_SAMPLES, len(dataset))\n",
    "        print(f\"Subsampling dataset to {n} examples for budget.\")\n",
    "        dataset = dataset.select(range(n))\n",
    "\n",
    "    def _map_fn(batch):\n",
    "        questions = batch[\"question\"]\n",
    "        answers = batch[\"answer\"]\n",
    "        documents = batch[\"documents\"]\n",
    "\n",
    "        texts = []\n",
    "        for q, a, docs in zip(questions, answers, documents):\n",
    "            ex = {\"question\": q, \"answer\": a, \"documents\": docs}\n",
    "            out = formatting_single_example(ex, tokenizer)\n",
    "            texts.append(out[\"text\"])\n",
    "\n",
    "        return {\"text\": texts}\n",
    "\n",
    "    print(\"Formatting dataset into chat-style text...\")\n",
    "    dataset = dataset.map(\n",
    "        _map_fn,\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names,\n",
    "        desc=\"Formatting prompts\",\n",
    "    )\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "031689df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model and tokenizer...\n",
      "==((====))==  Unsloth 2025.11.3: Fast Gemma3 patching. Transformers: 4.57.1.\n",
      "   \\\\   /|    NVIDIA L4. Num GPUs = 1. Max memory: 22.034 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Gemma3 does not support SDPA - switching to fast eager.\n",
      "Applying LoRA (QLoRA) configuration...\n",
      "Unsloth: Making `model.base_model.model.model.language_model` require gradients\n",
      "Loading dataset: FreedomIntelligence/RAG-Instruct\n",
      "Formatting dataset into chat-style text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Formatting prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40541/40541 [00:05<00:00, 7632.88 examples/s]\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_model_and_tokenizer()\n",
    "dataset = load_and_prepare_dataset(tokenizer)\n",
    "\n",
    "# Only use 80% of data for training, rest for eval\n",
    "train_size = int(0.8 * len(dataset))\n",
    "train_dataset = dataset.select(range(train_size))\n",
    "eval_dataset = dataset.select(range(train_size, len(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3eae169e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bfloat16 supported: True\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "bf16 = is_bfloat16_supported()\n",
    "print(f\"bfloat16 supported: {bf16}\")\n",
    "report_to = \"wandb\" if USE_WANDB else \"none\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ada31dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCS sync callback enabled.\n",
      "Starting SFT training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 32,432 | Num Epochs = 1 | Total steps = 8,108\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 4 x 1) = 4\n",
      " \"-____-\"     Trainable parameters = 14,901,248 of 4,314,980,720 (0.35% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8108' max='8108' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8108/8108 7:34:44, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.673700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.243800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.217400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.194700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.191400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.165900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.148600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.168800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.145600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.154300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>2.127000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>2.134600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>2.128500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>2.150200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.117400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>2.138900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>2.151600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>2.142200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>2.119300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.134500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>2.108300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>2.114400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>2.113700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>2.106600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.099800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>2.107100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>2.109900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>2.122800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>2.096700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>2.095600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>2.104400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>2.098700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>2.097300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>2.098100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>2.096900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>2.084100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>2.081300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>2.087700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>2.083700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>2.095500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n",
      "[GCSSync] Syncing /home/yashpareek_workmail/unsloth_finetuning/notebooks/outputs/gemma3-4b-rag to gs://model-finetune-1/checkpoints/gemma3-4b-rag ...\n",
      "[GCSSync] Sync complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8108, training_loss=2.1408763635199706, metrics={'train_runtime': 27333.9796, 'train_samples_per_second': 1.187, 'train_steps_per_second': 0.297, 'total_flos': 3.1683510571566246e+17, 'train_loss': 2.1408763635199706, 'epoch': 1.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    logging_steps=200,\n",
    "    save_steps=100,\n",
    "    save_total_limit=3,\n",
    "    bf16=bf16,\n",
    "    fp16=not bf16,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    report_to=report_to,\n",
    ")\n",
    "\n",
    "callbacks = []\n",
    "if ENABLE_GCS_SYNC:\n",
    "    gcs_callback = GCSSyncCallback(\n",
    "        local_dir=OUTPUT_DIR,\n",
    "        bucket_name=BUCKET_NAME,\n",
    "        prefix=GCS_PREFIX,\n",
    "    )\n",
    "    callbacks.append(gcs_callback)\n",
    "    print(\"GCS sync callback enabled.\")\n",
    "\n",
    "print(\"Starting SFT training...\")\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    packing=True,  # packs multiple samples per sequence -> good for throughput\n",
    "    args=training_args,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fa39616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to /home/yashpareek_workmail/unsloth_finetuning/notebooks/rag-instruct-gemma-3-finetuned\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "save_dir = Path(\"rag-instruct-gemma-3-finetuned\")\n",
    "\n",
    "save_dir.mkdir(exist_ok=True)\n",
    "\n",
    "model.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "print(\"Saved to\", save_dir.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbe467b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastModel\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "base_model_name = \"unsloth/gemma-3-4b-it\"\n",
    "adapter_dir = \"rag-instruct-gemma-3-finetuned\"\n",
    "\n",
    "base_model, base_tokenizer = FastModel.from_pretrained(\n",
    "    base_model_name,\n",
    "    max_seq_length=2048,\n",
    "    load_in_4bit=True,\n",
    "    load_in_8bit=False,\n",
    "    full_finetuning=False,\n",
    ")\n",
    "\n",
    "from peft import PeftModel\n",
    "model = PeftModel.from_pretrained(base_model, adapter_dir)\n",
    "tokenizer = base_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b82216a",
   "metadata": {},
   "source": [
    "## To get checkpoint to train again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d917d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_ckpt = get_last_checkpoint(OUTPUT_DIR)\n",
    "if last_ckpt is not None:\n",
    "    print(f\"Resuming from checkpoint: {last_ckpt}\")\n",
    "    trainer.train(resume_from_checkpoint=last_ckpt)\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting fresh.\")\n",
    "    trainer.train()\n",
    "\n",
    "print(\"Saving final adapter + tokenizer...\")\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth-venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
